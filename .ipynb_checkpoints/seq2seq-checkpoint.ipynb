{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import random\n",
    "import unicodedata\n",
    "import spacy\n",
    "import string\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import en_core_web_sm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text():\n",
    "    print(\"Reading training dataset from Stanford\")\n",
    "    lines = []\n",
    "    \n",
    "    with open('train.txt') as fp:\n",
    "        for line in fp:\n",
    "            line = re.sub(r\"(your persona:.*\\\\n)\", ' ', line)\n",
    "            line = ' '.join(line.split())   \n",
    "            question = re.findall(r\"text:(.*)labels:\", line)\n",
    "            answer = re.findall(r\"labels:(.*)(episode_done:)\", line)\n",
    "            if len(answer) == 0:\n",
    "                answer = re.findall(r\"labels:(.*)(question:)\", line)\n",
    "            if len(answer) and len(question):\n",
    "                lines.append(question[0])\n",
    "                lines.append(answer[0][0])\n",
    "    \n",
    "    return lines\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_tokenizer(nlp):\n",
    "    custom_prefixes = [r'[0-9]+', r'\\~', r'\\–', r'\\—', r'\\$']\n",
    "    custom_infixes = [r'[!&:,()]', r'\\.', r'\\-', r'\\–', r'\\—', r'\\$']\n",
    "    custom_suffixes = [r'\\.', r'\\–', r'\\—', r'\\$']\n",
    "    default_prefixes = list(nlp.Defaults.prefixes) + custom_prefixes\n",
    "    default_prefixes.remove(r'US\\$')\n",
    "    default_prefixes.remove(r'C\\$')\n",
    "    default_prefixes.remove(r'A\\$')\n",
    "    \n",
    "    all_prefixes_re = spacy.util.compile_prefix_regex(tuple(default_prefixes))\n",
    "    infix_re = spacy.util.compile_infix_regex(tuple(list(nlp.Defaults.infixes) + custom_infixes))\n",
    "    suffix_re = spacy.util.compile_suffix_regex(tuple(list(nlp.Defaults.suffixes) + custom_suffixes))\n",
    "\n",
    "    rules = dict(nlp.Defaults.tokenizer_exceptions)\n",
    "    # remove \"a.\" to \"z.\" rules so \"a.\" gets tokenized as a|.\n",
    "    for c in range(ord(\"a\"), ord(\"z\") + 1):\n",
    "        if f\"{chr(c)}.\" in rules:\n",
    "            rules.pop(f\"{chr(c)}.\")\n",
    "\n",
    "    return Tokenizer(nlp.vocab, rules,\n",
    "                     prefix_search=all_prefixes_re.search,\n",
    "                     infix_finditer=infix_re.finditer, suffix_search=suffix_re.search,\n",
    "                     token_match=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this special token to join the pre-tokenized data\n",
    "JOIN_TOKEN = \" \"\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.tokenizer = create_custom_tokenizer(nlp)\n",
    "\n",
    "def tokenize(text: string, tokenizer=nlp):\n",
    "    tokens = [tok for tok in nlp.tokenizer(text) if not tok.text.isspace()]\n",
    "    text_tokens = [tok.text for tok in tokens]\n",
    "    return tokens, text_tokens\n",
    "\n",
    "\n",
    "def tokenize_and_join(text: string, jointoken=JOIN_TOKEN):\n",
    "    return jointoken.join(tokenize(text)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize_and_join(\"Lazy fox doesn't like to travel too far in this heat...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 128\n",
    "\n",
    "def filter_pair(p):\n",
    "    return  len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def create_pairs(sentences):\n",
    "    pairs = []\n",
    "    for idx in range(0, len(sentences)-1, 2):\n",
    "        pair = [sentences[idx], sentences[idx+1]]\n",
    "        if filter_pair(pair):\n",
    "            pairs.append(pair)\n",
    "    return pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Person:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    data = read_text()\n",
    "    pairs = create_pairs(data)\n",
    "    print(\"data: \", data[:2])\n",
    "    print(\"pair: \", pairs[0])\n",
    "    input_data = Person(\"persona1\")\n",
    "    target_data = Person(\"persona2\")\n",
    "    for pair in pairs:\n",
    "        input_data.addSentence(pair[0])\n",
    "        target_data.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_data.name, input_data.n_words)\n",
    "    print(target_data.name, target_data.n_words)\n",
    "    return input_data, target_data, pairs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, target_data, pairs = prepare_data()\n",
    "test_input = \"hi how are you\"\n",
    "for c in test_input.split():\n",
    "    print(\"Word and Idx: \", c, input_data.word2index[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexes_from_sentence(data, sentence):\n",
    "    return [data.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensor_from_sentence(data, sentence):\n",
    "    indexes = indexes_from_sentence(data, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensors_from_pair(pair):\n",
    "    input_tensor = tensor_from_sentence(input_data, pair[0])\n",
    "    target_tensor = tensor_from_sentence(target_data, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return  1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input_text, hidden):\n",
    "        embedded = self.embedding(input_text).view(1,1,-1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size) #Applies a linear transformation to the incoming data\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, input_data, hidden):\n",
    "        output = self.embedding(input_data).view(1,1,-1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length = MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "    loss = 0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_hidden[0, 0]\n",
    "     \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder( decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach() #detach from history as input            \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item() / target_length\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iter(encoder, decoder, n_iters, print_every=1000, learning_rate = 0.01):\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate) #stochastic gradient descent \n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensors_from_pair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    print_loss_total = 0\n",
    "    \n",
    "    for it in range(1, n_iters+1):\n",
    "        training_pair = training_pairs[it-1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        \n",
    "        if it % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('(%d %d%%) %.4f' % (it, it / n_iters * 100, print_loss_avg))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder = EncoderRNN(input_data.n_words, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, target_data.n_words).to(device)\n",
    "#print(\"encoder: \", encoder)\n",
    "#print(\"decoder: \", decoder)\n",
    "train_iter(encoder, decoder, 50000, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensor_from_sentence(input_data, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "        \n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "            \n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(target_data.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
