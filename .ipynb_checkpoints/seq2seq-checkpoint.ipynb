{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext.data as data\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import random\n",
    "import unicodedata\n",
    "import spacy\n",
    "import string\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from torchtext.data import Field\n",
    "import en_core_web_sm\n",
    "import csv\n",
    "from torchtext.data import Dataset\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text():\n",
    "    print(\"Reading training dataset from Stanford\")\n",
    "    lines = []\n",
    "    with open('train.txt') as fp:\n",
    "        for line in fp:\n",
    "            line = re.sub(r\"(your persona:.*\\\\n)\", ' ', line)\n",
    "            line = ' '.join(line.split())   \n",
    "            question = re.findall(r\"text:(.*)labels:\", line)\n",
    "            answer = re.findall(r\"labels:(.*)(episode_done:)\", line)\n",
    "            if len(answer) == 0:\n",
    "                answer = re.findall(r\"labels:(.*)(question:)\", line)\n",
    "            if len(answer) and len(question):\n",
    "                lines.append(question[0])\n",
    "                lines.append(answer[0][0])\n",
    "    \n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_tokenizer(nlp):\n",
    "    custom_prefixes = [r'[0-9]+', r'\\~', r'\\–', r'\\—', r'\\$']\n",
    "    custom_infixes = [r'[!&:,()]', r'\\.', r'\\-', r'\\–', r'\\—', r'\\$']\n",
    "    custom_suffixes = [r'\\.', r'\\–', r'\\—', r'\\$']\n",
    "    default_prefixes = list(nlp.Defaults.prefixes) + custom_prefixes\n",
    "    default_prefixes.remove(r'US\\$')\n",
    "    default_prefixes.remove(r'C\\$')\n",
    "    default_prefixes.remove(r'A\\$')\n",
    "    \n",
    "    all_prefixes_re = spacy.util.compile_prefix_regex(tuple(default_prefixes))\n",
    "    infix_re = spacy.util.compile_infix_regex(tuple(list(nlp.Defaults.infixes) + custom_infixes))\n",
    "    suffix_re = spacy.util.compile_suffix_regex(tuple(list(nlp.Defaults.suffixes) + custom_suffixes))\n",
    "\n",
    "    rules = dict(nlp.Defaults.tokenizer_exceptions)\n",
    "    # remove \"a.\" to \"z.\" rules so \"a.\" gets tokenized as a|.\n",
    "    for c in range(ord(\"a\"), ord(\"z\") + 1):\n",
    "        if f\"{chr(c)}.\" in rules:\n",
    "            rules.pop(f\"{chr(c)}.\")\n",
    "\n",
    "    return Tokenizer(nlp.vocab, rules,\n",
    "                     prefix_search=all_prefixes_re.search,\n",
    "                     infix_finditer=infix_re.finditer, suffix_search=suffix_re.search,\n",
    "                     token_match=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this special token to join the pre-tokenized data\n",
    "JOIN_TOKEN = \" \"\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.tokenizer = create_custom_tokenizer(nlp)\n",
    "\n",
    "def tokenize(text: string, tokenizer=nlp):\n",
    "    tokens = [tok for tok in nlp.tokenizer(text) if not tok.text.isspace()]\n",
    "    text_tokens = [tok.text for tok in tokens]\n",
    "    return tokens, text_tokens\n",
    "\n",
    "\n",
    "def tokenize_and_join(text: string, jointoken=JOIN_TOKEN):\n",
    "    return jointoken.join(tokenize(text)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training dataset from Stanford\n",
      "262876\n",
      "hi , how are you doing ? i 'm getting ready to do some cheetah chasing to stay in shape .\n"
     ]
    }
   ],
   "source": [
    "lines = read_text()\n",
    "print(len(lines))\n",
    "print(tokenize_and_join(lines[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(name, from_line, to_line):\n",
    "    with open(name, mode='w') as csv_file:\n",
    "        fieldnames = ['question', 'answer']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for i in range(from_line, to_line,2):\n",
    "            writer.writerow({'question': lines[i], 'answer': lines[i+1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data('train.csv', 0, int(len(lines)*2/3))\n",
    "create_data('valid.csv', int(len(lines)*2/3),len(lines))\n",
    "create_data('test.csv', int(len(lines)/5), int(len(lines)/5*4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDataset(Dataset):\n",
    "    def __init__(self, sequences,**kwargs):\n",
    "        super(SDataset, self).__init__(self,sequences, sequences,**kwargs) \n",
    "    def prepare_fields():\n",
    "        TEXT = Field(sequential=True, tokenize=lambda s: str.split(s, sep=JOIN_TOKEN), lower=True)\n",
    "        return [\n",
    "            ('question',TEXT),\n",
    "            ('answer',TEXT)   \n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_batch_size': 5, 'embedding_size': 100}\n",
      "<torchtext.data.iterator.BucketIterator object at 0x7f686be1af98>\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "config = { \"train_batch_size\": 5, \"embedding_size\": 100}\n",
    "print(config)\n",
    "\n",
    "data_fields = SDataset.prepare_fields()\n",
    "fields = dict(tv_datafields)\n",
    "\n",
    "train = SDataset(fields)\n",
    "trn, vld, test = TabularDataset.splits(\n",
    "        path=\"~/nlg\",  # the root directory where the data lies\n",
    "        train='train.csv', validation=\"valid.csv\", test='test.csv',\n",
    "        format='csv',\n",
    "        skip_header=True,\n",
    "        # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "        fields=data_fields)\n",
    "\n",
    "fields[\"question\"].build_vocab(trn, vectors=GloVe(name='6B', dim=config[\"embedding_size\"]))\n",
    "\n",
    "vocab = fields[\"question\"].vocab\n",
    "\n",
    "train_iter = BucketIterator(train,\n",
    "                            shuffle=True, sort=True, sort_within_batch=True,\n",
    "                            sort_key=lambda x: len(x.question),\n",
    "                            batch_size=config[\"train_batch_size\"],\n",
    "                            repeat=False,\n",
    "                            device=device)\n",
    "print(train_iter)\n",
    "print(vocab.freqs.most_common(50))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'hi', ',', 'how', 'are']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0].__dict__.keys()\n",
    "trn[0].question[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter = BucketIterator.splits(\n",
    " (trn, vld), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_sizes=(64, 64),\n",
    " device=device, # if you want to use the GPU, specify the GPU number here\n",
    " sort_key=lambda x: len(x.question), # the BucketIterator needs to be told what function it should use to group the data.\n",
    " sort_within_batch=False,\n",
    " repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('question', <torchtext.data.field.Field object at 0x7f689169c198>), ('answer', <torchtext.data.field.Field object at 0x7f689169c198>)]\n"
     ]
    }
   ],
   "source": [
    "# create fields\n",
    "fields = SDataset.prepare_fields()\n",
    "print(fields)\n",
    "fields = dict(fields)\n",
    "train = SDataset(fields)\n",
    "trn, vld, test = TabularDataset.splits(\n",
    "    path=\"~/nlg\",  # the root directory where the data lies\n",
    "    train='train.csv', validation=\"valid.csv\", test='test.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "    fields=data_fields)\n",
    "fields[\"question\"].build_vocab(trn, vectors=GloVe(name='6B', dim=config[\"embedding_size\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 128\n",
    "\n",
    "def filter_pair(p):\n",
    "    return  len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def create_pairs(sentences):\n",
    "    pairs = []\n",
    "    for idx in range(0, len(sentences)-1, 2):\n",
    "        pair = [sentences[idx], sentences[idx+1]]\n",
    "        if filter_pair(pair):\n",
    "            pairs.append(pair)\n",
    "    return pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Person:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    data = read_text()\n",
    "    pairs = create_pairs(data)\n",
    "    print(\"data: \", data[:2])\n",
    "    print(\"pair: \", pairs[0])\n",
    "    input_data = Person(\"persona1\")\n",
    "    target_data = Person(\"persona2\")\n",
    "    for pair in pairs:\n",
    "        input_data.addSentence(pair[0])\n",
    "        target_data.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_data.name, input_data.n_words)\n",
    "    print(target_data.name, target_data.n_words)\n",
    "    return input_data, target_data, pairs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training dataset from Stanford\n",
      "data:  [\" hi , how are you doing ? i'm getting ready to do some cheetah chasing to stay in shape . \", 'you must be very fast . hunting is one of my favorite hobbies . ']\n",
      "pair:  [\" hi , how are you doing ? i'm getting ready to do some cheetah chasing to stay in shape . \", 'you must be very fast . hunting is one of my favorite hobbies . ']\n",
      "Counted words:\n",
      "persona1 18081\n",
      "persona2 18733\n",
      "Word and Idx:  hi 3\n",
      "Word and Idx:  how 5\n",
      "Word and Idx:  are 6\n",
      "Word and Idx:  you 7\n"
     ]
    }
   ],
   "source": [
    "input_data, target_data, pairs = prepare_data()\n",
    "test_input = \"hi how are you\"\n",
    "for c in test_input.split():\n",
    "    print(\"Word and Idx: \", c, input_data.word2index[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexes_from_sentence(data, sentence):\n",
    "    return [data.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensor_from_sentence(data, sentence):\n",
    "    indexes = indexes_from_sentence(data, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensors_from_pair(pair):\n",
    "    input_tensor = tensor_from_sentence(input_data, pair[0])\n",
    "    target_tensor = tensor_from_sentence(target_data, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return  1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input_text, hidden):\n",
    "        embedded = self.embedding(input_text).view(1,1,-1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size) #Applies a linear transformation to the incoming data\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, input_data, hidden):\n",
    "        output = self.embedding(input_data).view(1,1,-1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length = MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "    loss = 0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_hidden[0, 0]\n",
    "     \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder( decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach() #detach from history as input            \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item() / target_length\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iter(encoder, decoder, n_iters, print_every=1000, learning_rate = 0.01):\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate) #stochastic gradient descent \n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensors_from_pair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    print_loss_total = 0\n",
    "    \n",
    "    for it in range(1, n_iters+1):\n",
    "        training_pair = training_pairs[it-1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        \n",
    "        if it % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('(%d %d%%) %.4f' % (it, it / n_iters * 100, print_loss_avg))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder = EncoderRNN(input_data.n_words, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, target_data.n_words).to(device)\n",
    "#print(\"encoder: \", encoder)\n",
    "#print(\"decoder: \", decoder)\n",
    "train_iter(encoder, decoder, 50000, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensor_from_sentence(input_data, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "        \n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "            \n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(target_data.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
